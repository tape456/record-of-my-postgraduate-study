# RL基本原理

强化学习的两类思路：

一个是基于值，另一个是基于策略：

![image-20251125173841701](RL基本原理-img/image-20251125173841701.png)

### 时序差分(Temporal Difference)TD算法

我们的目的就是去找到一种方法能够`估计`在一个状态下做出动作，他的一个价值。

![image-20251125174327972](RL基本原理-img/image-20251125174327972.png)



![image-20251125185222517](RL基本原理-img/image-20251125185222517.png)

蒙特卡洛：是一长串多步估计，没有去考虑。

我玩完这一局游戏，我这里面不是有很多随机性吗，但我玩完它这个就确定下来，你没有去考虑它那么多期望、随机性，你就是相当于做了一个很多采样，我们知道很多变量它本身就有方差，如果把很多变量的方差叠加起来，他这个方差肯定是会很大的，但是因为你采取了这么一整条轨迹，我可以对它的return做一个很精确的一个计算，所以偏差可能会小。

![image-20251125190646846](RL基本原理-img/image-20251125190646846.png)

时序差分：相比于蒙特卡洛，他的方差会比较小，但是他的偏差可能就会比较大，因为它就是对一步做了一个估计。



TD算法有两种应用：一种叫SARSA，一种叫Q-learning















# 动手学强化学习

## 基础篇

关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。

**占用度量**：归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到**一个具体的状态动作对**（state-action pair）的**概率分布**。

* 强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。

* 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示**最小化**模型在整个数据分布上的泛化误差（generalization error），用简要的公式可以概括为：

![image-20260104214509383](RL基本原理-img/image-20260104214509383.png)

强化学习任务的最终优化目标是**最大化**智能体策略在和动态环境交互过程中的价值。策略的价值可以等价转换成**奖励函数在策略的占用度量上的期望**，即：

![](RL基本原理-img/image-20260104214642777.png)



* 有监督学习和强化学习的优化目标相似，即都是在**优化某个数据分布下的一个分数值的期望**。

* 二者**优化的途径是不同的**:

  `有监督学习`直接通过优化模型对于数据特征的输出来优化目标，即`修改目标函数而数据分布不变`；

  `强化学习`则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即`修改数据分布而目标函数不变`。

一般有监督学习和强化学习的范式之间的区别为：

- 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；

- 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。

### 多臂老虎机（multi-armed bandit，MAB）问题

与强化学习不同，多臂老虎机`不存在状态信息`，只有动作和奖励，算是最简单的“和环境交互中的学习”的一种形式。

多臂老虎机中的**探索与利用**（exploration vs. exploitation）问题一直以来都是一个特别经典的问题，理解它能够帮助我们学习强化学习。

估计期望奖励更新算法流程：

![image-20260105163542778](RL基本原理-img/image-20260105163542778.png)

第四步的增量式的期望更新公式如下：

![image-20260105163636533](RL基本原理-img/image-20260105163636533.png)

如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为O(n),而采用增量式更新，时间复杂度和空间复杂度均为O(1)。



#### ε-贪心算法

以概率ε随机选择一根拉杆（探索）

选择以往经验中期望奖励估值最大的那根拉杆（利用）1-ε

![image-20260105211242986](RL基本原理-img/image-20260105211242986.png)

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。

具体实现中令ε随时间衰减，即探索的概率将会不断降低。

#### 上置信界算法（upper confidence bound，UCB）

上置信界算法便选取期望奖励上界最大的动作

![image-20260105211638786](RL基本原理-img/image-20260105211638786.png)

不确定性度量U(a)

#### 汤普森采样算法

之前我们知道了 “用抽样选最优拉杆”，但得先知道 “每台老虎机的赢钱概率大概是多少”，还得拉一次就更新一次这个概率 —— 这段就是讲 “怎么记录 + 更新每台老虎机的赢钱概率”。

**核心点 1：用 Beta 分布当 “概率记录本”**

不用管 “Beta 分布” 的复杂数学，你可以把它理解成：**专门用来表示 “某台老虎机赢钱概率的可能性范围” 的工具**。

比如某台机器的 Beta 分布，能体现 “它赢钱概率是 30%？还是 50%？哪种可能性更大”。

**核心点 2：怎么用 “拉拉杆的输赢记录” 填这个 “记录本”**

假设你拉了某台老虎机`k次`：

- 其中`m₁次赢了`（奖励是 1）

- m₂次输了（奖励是 0）

  （其实 k 就是 m₁+m₂，比如拉 5 次，赢 3 次输 2 次，k=5，m₁=3，m₂=2）

这台老虎机的 “赢钱概率范围”，就用**参数是 (m₁+1, m₂+1) 的 Beta 分布**来表示。

比如上面的例子，参数就是 (3+1, 2+1)=(4,3)—— 这个 Beta 分布就能反映 “这台机实际赢钱概率的可能性”。

**核心点 3：怎么更新这个 “记录本”**

每次拉这台老虎机，都记录 “赢了还是输了”：

- 要是赢了，就把 m₁加 1，参数里的第一个数也跟着加 1；
- 要是输了，就把 m₂加 1，参数里的第二个数也跟着加 1。

拉的次数越多，这个 Beta 分布就越接近这台老虎机的**真实赢钱概率**（相当于 “记录本” 越记越准）。

![image-20260105214843255](RL基本原理-img/image-20260105214843255.png)

不同颜色的曲线（Q1 粉、Q2 蓝、Q3 绿）：对应**不同的老虎机（不同拉杆）**的 “赢钱概率可能性分布”（也就是之前说的 Beta 分布）。

你看每条曲线旁边的点（比如 Q1 的点在 0.5 附近，Q3 的点在 0.75 附近），其实是**从这台老虎机的概率分布里 “抽的一个样本”**（比如从 Q3 的分布里抽到了 0.75 这个赢钱概率）。

汤普森采样就是：从每台机器的曲线里抽一个样本，选样本里赢钱概率最大的那台（比如这次 Q3 的样本最大，就拉 Q3 对应的老虎机）。

#### 总结

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。



### 马尔可夫决策过程（MDP）

**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。

如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

本章将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。

#### 马尔可夫过程（马尔可夫链）

##### 随机过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。

##### 马尔可夫性质

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为

![image-20260106112246265](RL基本原理-img/image-20260106112246265.png)

通过这种链式的关系，历史的信息被传递到了现在。只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

##### 马尔可夫过程（马尔可夫链）

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。

通常用元组![image-20260106113450501](RL基本原理-img/image-20260106113450501.png)描述一个马尔可夫过程，S是有限数量的状态集合，P是**状态转移矩阵**（state transition matrix)。

![image-20260106115623590](RL基本原理-img/image-20260106115623590.png)

![image-20260106120243291](RL基本原理-img/image-20260106120243291.png)

#### 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到**马尔可夫奖励过程**（Markov reward process）一个马尔可夫奖励过程由![image-20260106120825956](RL基本原理-img/image-20260106120825956.png)构成

![image-20260106120856880](RL基本原理-img/image-20260106120856880.png)

##### 回报













