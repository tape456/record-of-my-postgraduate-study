# RL基本原理

强化学习的两类思路：

一个是基于值，另一个是基于策略：

![image-20251125173841701](RL基本原理-img/image-20251125173841701.png)

### 时序差分(Temporal Difference)TD算法

我们的目的就是去找到一种方法能够`估计`在一个状态下做出动作，他的一个价值。

![image-20251125174327972](RL基本原理-img/image-20251125174327972.png)



![image-20251125185222517](RL基本原理-img/image-20251125185222517.png)

蒙特卡洛：是一长串多步估计，没有去考虑。

我玩完这一局游戏，我这里面不是有很多随机性吗，但我玩完它这个就确定下来，你没有去考虑它那么多期望、随机性，你就是相当于做了一个很多采样，我们知道很多变量它本身就有方差，如果把很多变量的方差叠加起来，他这个方差肯定是会很大的，但是因为你采取了这么一整条轨迹，我可以对它的return做一个很精确的一个计算，所以偏差可能会小。

![image-20251125190646846](RL基本原理-img/image-20251125190646846.png)

时序差分：相比于蒙特卡洛，他的方差会比较小，但是他的偏差可能就会比较大，因为它就是对一步做了一个估计。



TD算法有两种应用：一种叫SARSA，一种叫Q-learning















# 动手学强化学习

## 基础篇

关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。

**占用度量**：归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到**一个具体的状态动作对**（state-action pair）的**概率分布**。

* 强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。

* 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示**最小化**模型在整个数据分布上的泛化误差（generalization error），用简要的公式可以概括为：

![image-20260104214509383](RL基本原理-img/image-20260104214509383.png)

强化学习任务的最终优化目标是**最大化**智能体策略在和动态环境交互过程中的价值。策略的价值可以等价转换成**奖励函数在策略的占用度量上的期望**，即：

![](RL基本原理-img/image-20260104214642777.png)



* 有监督学习和强化学习的优化目标相似，即都是在**优化某个数据分布下的一个分数值的期望**。

* 二者**优化的途径是不同的**:

  `有监督学习`直接通过优化模型对于数据特征的输出来优化目标，即`修改目标函数而数据分布不变`；

  `强化学习`则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即`修改数据分布而目标函数不变`。

一般有监督学习和强化学习的范式之间的区别为：

- 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；

- 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。

### 2.多臂老虎机（multi-armed bandit，MAB）问题

与强化学习不同，多臂老虎机`不存在状态信息`，只有动作和奖励，算是最简单的“和环境交互中的学习”的一种形式。

多臂老虎机中的**探索与利用**（exploration vs. exploitation）问题一直以来都是一个特别经典的问题，理解它能够帮助我们学习强化学习。

估计期望奖励更新算法流程：

![image-20260105163542778](RL基本原理-img/image-20260105163542778.png)

第四步的增量式的期望更新公式如下：

![image-20260105163636533](RL基本原理-img/image-20260105163636533.png)

如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为O(n),而采用增量式更新，时间复杂度和空间复杂度均为O(1)。



#### ε-贪心算法

以概率ε随机选择一根拉杆（探索）

选择以往经验中期望奖励估值最大的那根拉杆（利用）1-ε

![image-20260105211242986](RL基本原理-img/image-20260105211242986.png)

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。

具体实现中令ε随时间衰减，即探索的概率将会不断降低。

#### 上置信界算法（upper confidence bound，UCB）

上置信界算法便选取期望奖励上界最大的动作

![image-20260105211638786](RL基本原理-img/image-20260105211638786.png)

不确定性度量U(a)

#### 汤普森采样算法

之前我们知道了 “用抽样选最优拉杆”，但得先知道 “每台老虎机的赢钱概率大概是多少”，还得拉一次就更新一次这个概率 —— 这段就是讲 “怎么记录 + 更新每台老虎机的赢钱概率”。

**核心点 1：用 Beta 分布当 “概率记录本”**

不用管 “Beta 分布” 的复杂数学，你可以把它理解成：**专门用来表示 “某台老虎机赢钱概率的可能性范围” 的工具**。

比如某台机器的 Beta 分布，能体现 “它赢钱概率是 30%？还是 50%？哪种可能性更大”。

**核心点 2：怎么用 “拉拉杆的输赢记录” 填这个 “记录本”**

假设你拉了某台老虎机`k次`：

- 其中`m₁次赢了`（奖励是 1）

- m₂次输了（奖励是 0）

  （其实 k 就是 m₁+m₂，比如拉 5 次，赢 3 次输 2 次，k=5，m₁=3，m₂=2）

这台老虎机的 “赢钱概率范围”，就用**参数是 (m₁+1, m₂+1) 的 Beta 分布**来表示。

比如上面的例子，参数就是 (3+1, 2+1)=(4,3)—— 这个 Beta 分布就能反映 “这台机实际赢钱概率的可能性”。

**核心点 3：怎么更新这个 “记录本”**

每次拉这台老虎机，都记录 “赢了还是输了”：

- 要是赢了，就把 m₁加 1，参数里的第一个数也跟着加 1；
- 要是输了，就把 m₂加 1，参数里的第二个数也跟着加 1。

拉的次数越多，这个 Beta 分布就越接近这台老虎机的**真实赢钱概率**（相当于 “记录本” 越记越准）。

![image-20260105214843255](RL基本原理-img/image-20260105214843255.png)

不同颜色的曲线（Q1 粉、Q2 蓝、Q3 绿）：对应**不同的老虎机（不同拉杆）**的 “赢钱概率可能性分布”（也就是之前说的 Beta 分布）。

你看每条曲线旁边的点（比如 Q1 的点在 0.5 附近，Q3 的点在 0.75 附近），其实是**从这台老虎机的概率分布里 “抽的一个样本”**（比如从 Q3 的分布里抽到了 0.75 这个赢钱概率）。

汤普森采样就是：从每台机器的曲线里抽一个样本，选样本里赢钱概率最大的那台（比如这次 Q3 的样本最大，就拉 Q3 对应的老虎机）。

#### 总结

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。



### 3.马尔可夫决策过程（MDP）

**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。

如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

本章将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。

#### 马尔可夫过程（马尔可夫链）

##### 随机过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。

##### 马尔可夫性质

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为

![image-20260106112246265](RL基本原理-img/image-20260106112246265.png)

`需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系`。通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

##### 马尔可夫过程（马尔可夫链）

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。

通常用元组![image-20260106113450501](RL基本原理-img/image-20260106113450501.png)描述一个马尔可夫过程，S是有限数量的状态集合，P是**状态转移矩阵**（state transition matrix)。

![image-20260106115623590](RL基本原理-img/image-20260106115623590.png)

![image-20260106120243291](RL基本原理-img/image-20260106120243291.png)

#### 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到**马尔可夫奖励过程**（Markov reward process）一个马尔可夫奖励过程由![image-20260106120825956](RL基本原理-img/image-20260106120825956.png)构成

![image-20260106120856880](RL基本原理-img/image-20260106120856880.png)

##### 回报

![image-20260119192949737](RL基本原理-img/image-20260119192949737.png)

核心是实现**折扣累计回报**的递推计算，反向遍历 + 递推公式是高效实现的关键。



##### 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。

一方面，即时奖励的期望正是奖励函数的输出；

![image-20260119193957349](RL基本原理-img/image-20260119193957349.png)

`状态 s 的价值 = 当前状态的即时奖励期望 + 未来所有可能转移到的状态的价值的折扣期望`

![image-20260119194529994](RL基本原理-img/image-20260119194529994.png)

因此，整个向量 V 作为未知量，会同时出现在方程的左右两边 —— 左边是我们要求的价值，右边是计算价值时用到的未来价值，而未来价值就是我们要求的价值本身。

![image-20260119194658626](RL基本原理-img/image-20260119194658626.png)



#### 马尔可夫决策过程

讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**（Markov decision process，MDP）。

我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。

![image-20260119200828100](RL基本原理-img/image-20260119200828100.png)

##### 策略

顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。

##### 状态价值函数

![image-20260119201819193](RL基本原理-img/image-20260119201819193.png)

##### 动作价值函数

![image-20260119201913405](RL基本原理-img/image-20260119201913405.png)

![image-20260119201937889](RL基本原理-img/image-20260119201937889.png)

![image-20260119202002221](RL基本原理-img/image-20260119202002221.png)

##### 贝尔曼期望方程

在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。我们通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）：

![image-20260119202100677](RL基本原理-img/image-20260119202100677.png)

价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的，读者需要明确掌握！

![image-20260119202254246](RL基本原理-img/image-20260119202254246.png)

接下来我们编写代码来表示图 3-4 中的马尔可夫决策过程，并定义两个策略。第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在下，智能体会以 0.5 和 0.5 的概率选取动作“保持”和“前往”。第二个策略是一个提前设定的一个策略。

于是，一个很自然的想法是：给定一个 MDP 和一个策略，我们是否可以将其转化为一个 MRP？答案是肯定的。我们可以将策略的动作选择进行**边缘化**（marginalization)，就可以得到没有动作的 MRP 了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个 MRP 在该状态下的奖励，即：

![image-20260119210049787](RL基本原理-img/image-20260119210049787.png)

#### 蒙特卡洛方法

```python
# 对所有采样序列计算所有状态的价值
def MC(episodes, V, N, gamma):
    for episode in episodes:
        G = 0
        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算
            (s, a, r, s_next) = episode[i]
            G = r + gamma * G
            N[s] = N[s] + 1
            V[s] = V[s] + (G - V[s]) / N[s]


timestep_max = 20
# 采样1000次,可以自行修改
episodes = sample(MDP, Pi_1, timestep_max, 1000)
gamma = 0.5
V = {"s1": 0, "s2": 0, "s3": 0, "s4": 0, "s5": 0}
N = {"s1": 0, "s2": 0, "s3": 0, "s4": 0, "s5": 0}
MC(episodes, V, N, gamma)
print("使用蒙特卡洛方法计算MDP的状态价值为\n", V)
```

采样 1000 条轨迹：`episodes = sample(...)`，样本量越大，根据「大数定律」，价值估计结果越接近 MDP 的真实状态价值（误差越小）。

方法类型：这段代码是**每次访问蒙特卡洛（Every-Visit MC）**，轨迹中每个状态每出现一次，就更新一次价值，样本利用率高于首次访问 MC。



#### 占用度量

我们还可以定义策略的**占用度量**（occupancy measure）。

![image-20260119215933419](RL基本原理-img/image-20260119215933419.png)

![image-20260119215942518](RL基本原理-img/image-20260119215942518.png)

![image-20260119220001019](RL基本原理-img/image-20260119220001019.png)

![image-20260119220009347](RL基本原理-img/image-20260119220009347.png)

注意：以上提到的“合法”占用度量是指存在一个策略使智能体与 MDP 交互产生的状态动作对被访问到的概率。

接下来我们编写代码来近似估计占用度量。这里我们采用近似估计，即设置一个较大的采样轨迹长度的最大值，然后采样很多次，用状态动作对出现的频率估计实际概率。

占用度量的核心价值是**量化某个策略下，特定（状态 - 动作对）被 “使用” 的频繁程度**

#### 最优策略

![image-20260119221217407](RL基本原理-img/image-20260119221217407.png)

最优策略都有相同的状态价值函数，我们称之为**最优状态价值函数**，表示为：

![image-20260119221259579](RL基本原理-img/image-20260119221259579.png)

同理，我们定义**最优动作价值函数**:

![image-20260119221312251](RL基本原理-img/image-20260119221312251.png)

![image-20260119221352343](RL基本原理-img/image-20260119221352343.png)

这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：

![image-20260119221411033](RL基本原理-img/image-20260119221411033.png)

##### 贝尔曼最优方程

![image-20260119221454202](RL基本原理-img/image-20260119221454202.png)

#### 总结

本章从零开始介绍了马尔可夫决策过程的基础概念知识，并讲解了如何通过求解贝尔曼方程得到状态价值的解析解以及如何用蒙特卡洛方法估计各个状态的价值。马尔可夫决策过程是强化学习中的基础概念，`强化学习中的环境就是一个马尔可夫决策过程`。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。



### 4.动态规划算法

动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。

**策略迭代中的策略评估**使用`贝尔曼期望方程`来得到一个策略的`状态价值函数`，这是一个动态规划的过程；而**价值迭代**直接使用`贝尔曼最优方程`来进行动态规划，得到最终的`最优状态价值`。



#### 悬崖漫步环境

悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如图 4-1 所示，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。

![image-20260126173307833](RL基本原理-img/image-20260126173307833.png)



#### 策略迭代算法

![image-20260126212805681](RL基本原理-img/image-20260126212805681.png)

![image-20260126212759236](RL基本原理-img/image-20260126212759236.png)

##### 策略评估

策略评估这一过程用来计算一个策略的状态价值函数。回顾一下之前学习的贝尔曼期望方程：

![image-20260126181303926](RL基本原理-img/image-20260126181303926.png)

更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即

![image-20260126181347102](RL基本原理-img/image-20260126181347102.png)

![image-20260126181507427](RL基本原理-img/image-20260126181507427.png)

##### 策略提升

使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。

![image-20260126200640275](RL基本原理-img/image-20260126200640275.png)这便是**策略提升定理**（policy improvement theorem)。

于是我们可以直接贪心地在每一个状态选择动作价值最大的动作，也就是

![image-20260126200706571](RL基本原理-img/image-20260126200706571.png)

![image-20260126200857164](RL基本原理-img/image-20260126200857164.png)

**策略提升定理的证明**通过以下推导过程可以证明，使用上述提升公式得到的`新策略在每个状态的价值不低于原策略在该状态的价值`。

![image-20260126200950390](RL基本原理-img/image-20260126200950390.png)

![image-20260126201125041](RL基本原理-img/image-20260126201125041.png)

##### 策略迭代算法

策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略（收敛性证明参见 4.7 节）：

**策略评估（Policy Evaluation）**：在当前策略下，精确计算每个状态的价值函数 `V(s)`。

**策略提升（Policy Improvement）**：用当前价值函数更新策略，让策略变得更优。

![image-20260126201429013](RL基本原理-img/image-20260126201429013.png)

结合策略评估和策略提升，我们得到以下策略迭代算法：

![image-20260126201531704](RL基本原理-img/image-20260126201531704.png)

#### 价值迭代算法

从上面的代码运行结果中我们能发现，策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。

可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数。

价值迭代可以看成一种动态规划过程，它利用的是贝尔曼最优方程：

![image-20260126205015198](RL基本原理-img/image-20260126205015198.png)

将其写成迭代更新的方式为：

![image-20260126205033082](RL基本原理-img/image-20260126205033082.png)

![image-20260126205045209](RL基本原理-img/image-20260126205045209.png)

价值迭代算法流程如下：

![image-20260126205125976](RL基本原理-img/image-20260126205125976.png)



#### 冰湖环境

除了悬崖漫步环境，本章还准备了另一个环境——**冰湖**（Frozen Lake）。冰湖环境的状态空间和动作空间是有限的，我们在该环境中也尝试一下策略迭代算法和价值迭代算法，以便更好地理解这两个算法。

冰湖是 OpenAI Gym 库中的一个环境。OpenAI Gym 库中包含了很多有名的环境，例如 Atari 和 MuJoCo，并且支持我们定制自己的环境。

```
策略评估进行25轮后完成
策略提升完成
策略评估进行58轮后完成
策略提升完成
状态价值：
 0.069  0.061  0.074  0.056
 0.092  0.000  0.112  0.000
 0.145  0.247  0.300  0.000
 0.000  0.380  0.639  0.000
策略：
<ooo ooo^ <ooo ooo^
<ooo **** <o>o ****
ooo^ ovoo <ooo ****
**** oo>o ovoo EEEE
```

这个最优策略很看上去比较反直觉，其原因是这是一个智能体会随机滑向其他状态的冰冻湖面。例如，在目标左边一格的状态，采取向右的动作时，它有可能会滑到目标左上角的位置，从该位置再次到达目标会更加困难，所以此时采取向下的动作是更为保险的，并且有一定概率能够滑到目标。我们再来尝试一下价值迭代算法。

```
价值迭代一共进行60轮
状态价值：
 0.069  0.061  0.074  0.056
 0.092  0.000  0.112  0.000
 0.145  0.247  0.300  0.000
 0.000  0.380  0.639  0.000
策略：
<ooo ooo^ <ooo ooo^
<ooo **** <o>o ****
ooo^ ovoo <ooo ****
**** oo>o ovoo EEEE
```



#### 小结

本章讲解了强化学习中两个经典的动态规划算法：策略迭代算法和价值迭代算法，它们都能用于求解最优价值和最优策略。动态规划的主要思想是利用贝尔曼方程对所有状态进行更新。需要注意的是，在利用贝尔曼方程进行状态更新时，我们会用到马尔可夫决策过程中的奖励函数和状态转移函数。如果智能体无法事先得知奖励函数和状态转移函数，就只能通过和环境进行交互来采样（状态-动作-奖励-下一状态）这样的数据，我们将在之后的章节中讲解如何求解这种情况下的最优策略。





### 5.时序差分算法

#### 简介

第 4 章介绍的动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。

但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就`无法直接进行动态规划`。

在这种情况下，`智能体只能和环境进行交互，通过采样到的数据来学习`，这类学习方法统称为**无模型的强化学习**（model-free reinforcement learning）。

不同于动态规划算法，**无模型的强化学习算法**`不需要事先知道环境的奖励函数和状态转移函数`，而是**直接使用和环境交互的过程中采样到的数据来学习**，这使得它可以被应用到一些简单的实际场景中。

本章将要讲解无模型的强化学习中的两大经典算法：Sarsa 和 Q-learning，它们都是基于**时序差分**（temporal difference，TD）的强化学习算法。

同时，本章还会引入一组概念：在线策略学习和离线策略学习。通常来说，`在线策略学习`要求使用在**当前策略下采样得到的样本**进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；`离线策略学习`使用**经验回放池**将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。

因此，`离线策略学习`往往能够更好地利用历史数据，并具有**更小的样本复杂度**（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。

#### 时序差分方法

**时序差分**是一种用来**估计一个策略的价值函数**的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和`动态规划的相似之处`在于根据贝尔曼方程的思想，`利用后续状态的价值估计`来更新`当前状态的价值估计`。

蒙特卡洛的核心思想，就像**通过多次重复做一件事，用平均结果来估计真实情况**。

回顾一下蒙特卡洛方法对价值函数的增量更新方式：

![image-20260128213027653](RL基本原理-img/image-20260128213027653.png)

蒙特卡洛的设计原则是 **“用已经发生的完整经验来学习”**，它拒绝任何 “对未来的估计”，只相信已经结束的、确定的事实。

![image-20260128212950680](RL基本原理-img/image-20260128212950680.png)

蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报，而时序差分方法只需要当前步结束即可进行计算。

具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：

![image-20260128213149267](RL基本原理-img/image-20260128213149267.png)

![image-20260128213244382](RL基本原理-img/image-20260128213244382.png)

状态价值的期望等于 “当前奖励 + 下一个状态价值的折扣期望”。

因此蒙特卡洛方法将上式第一行作为更新的目标，而时序差分算法将上式最后一行作为更新的目标。

于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了**V**(St+1)的估计值，可以证明它最终收敛到策略Π的价值函数，我们在此不对此进行展开说明。



#### Sarsa 算法

既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。

策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是可以直接用时序差分算法来估计动作价值函数Q:

![image-20260130162950154](RL基本原理-img/image-20260130162950154.png)

然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即![image-20260130163053541](RL基本原理-img/image-20260130163053541.png)。

这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新`动作价值估计`。

然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们`需要用极大量的样本来进行更新`。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。`误差的积累需要大量样本 “平均掉”`

因为 TD 的每一步更新都依赖 “有误差的估计”，单个样本的信息非常有限，甚至可能引入噪声。

- 比如你玩马里奥，某一步跳到平台上拿到了 + 10 分，但下一个状态 st+1 的价值估计其实是不准的（因为它也是之前用 TD 更新出来的）。
- 这一次更新会给 V(st) 带来偏差，但如果我们重复玩很多次（用大量样本），这些随机的误差就会被**平均掉**，`让价值估计逐渐收敛到真实值`。

==但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。==

每走一步、收集一个样本，就立刻更新价值函数，然后基于最新的价值去调整策略。

这些算法在实际场景中（比如游戏、机器人控制）都能快速找到优秀的策略，证明了 “不用等大量样本” 的做法是可行的。这其实是**广义策略迭代**（generalized policy iteration）的思想。



第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。我们在第 2 章中对此有详细讨论。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个-贪婪策略：有的概率采用动作价值最大的那个动作，另外有的概率从动作空间中随机采取一个动作，其公式表示为：

![image-20260130165222407](RL基本原理-img/image-20260130165222407.png)

现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态、当前动作、获得的奖励、下一个状态和下一个动作，将这些符号拼接后就得到了算法名称。Sarsa 的具体算法如下:

![image-20260130165258372](RL基本原理-img/image-20260130165258372.png)

##### 逐行拆解（结合游戏例子）

###### 🔹 初始化 Q(s,a)

- 意思：最开始，你对所有 “状态 - 动作” 对的价值完全没概念，所以先把所有 Q(s,a) 都设为 0。
- 游戏例子：刚打开新游戏，你不知道 “在起点走” 和 “在起点跳” 哪个更好，就默认它们价值都是 0。

###### 外层循环：`for 序列e = 1 → E do`

- 意思：我们要完整地玩 E 局游戏（比如玩 100 局），每一局都从头开始。

- 得到初始状态 **s**

  - 游戏例子：每局开始，你都站在 “起点” 这个初始状态。

  

- 用**ε**-greedy 策略选择动作**a**

  - 意思：这是 “探索 + 利用” 的平衡策略：
    - 大部分时候（比如 90% 概率），选当前价值最高的动作（**利用**已知的好选择）。
    - 偶尔（比如 10% 概率），随机选一个动作（**探索**新的可能性）。
  - 游戏例子：在起点时，Q 值都是 0，所以随机选了 “走” 这个动作。

###### 🔹 内层循环：`for 时间步t = 1 → T do`

- 意思：这是单局游戏里的每一步，直到游戏结束（比如掉坑或通关）。

- 得到环境反馈的**r,s′**

  - 意思：你执行动作 a 后，环境会给你一个即时奖励 r，同时让你进入新状态 s′。
  - 游戏例子：你 “走” 了一步，吃到 1 个金币（奖励 r=+5），并进入了 “金币位置” 这个新状态 s′。

- 用**ε**-greedy 策略选择新动作**a′**

  - 意思：在新状态 s′ 下，同样用 ε-greedy 选下一个动作。
  - 游戏例子：在 “金币位置”，Q 值都是 0，随机选了 “跳” 这个动作 a′。

- 更新**Q(s,a)**的核心公式

  Q(s,a)←Q(s,a)+α[r+γQ(s′,a′)−Q(s,a)]

  我们拆解成游戏里的数字（假设 α=0.1，γ=0.9）：

  - 初始 起点走
  - 计算 TD 目标：金币位置跳
  - 计算 TD 误差：5−0=5
  - 更新 Q 值：起点走
  - 意思：现在 “起点 - 走” 的价值从 0 变成了 0.5，因为它让你拿到了 5 分。

- **s←s′,a←a′**

  - 意思：把当前状态和动作更新为新的 s′ 和 a′，准备下一步。
  - 游戏例子：现在你的状态变成 “金币位置”，动作变成 “跳”，继续玩下一步。

###### 结束内层循环 & 外层循环

- 意思：当一局游戏结束（比如掉坑），就跳出内层循环，开始下一局游戏，重复整个流程。
- 玩的局数越多，Q 值就越准，你选的动作也会越来越厉害，最终能稳定拿到高分。

###### Sarsa 的核心特点

1. **在线学习**：用来选动作的策略（ε-greedy）和用来更新价值的策略是同一个。
2. **一步更新**：不用等整局结束，走一步就更新一次 Q 值，效率很高。
3. **探索与利用平衡**：通过 ε-greedy 保证你不会一直用老动作，会偶尔尝试新动作。



```python
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm  # tqdm是显示循环进度条的库


class CliffWalkingEnv:
    def __init__(self, ncol, nrow):
        self.nrow = nrow
        self.ncol = ncol
        self.x = 0  # 记录当前智能体位置的横坐标
        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标

    def step(self, action):  # 外部调用这个函数来改变当前位置
        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)
        # 定义在左上角
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))
        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))
        next_state = self.y * self.ncol + self.x
        reward = -1
        done = False
        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标
            done = True
            if self.x != self.ncol - 1:
                reward = -100
        return next_state, reward, done

    def reset(self):  # 回归初始状态,坐标轴原点在左上角
        self.x = 0
        self.y = self.nrow - 1
        return self.y * self.ncol + self.x
```



==二维坐标转一维状态值==：

```python
next_state = self.y * self.ncol + self.x
```

 是强化学习的常用技巧

返回行动结果:

给外部算法返回 3 个关键信息：`next_state`（新状态）、`reward`（即时奖励）、`done`（任务是否结束），供算法更新 Q 值。

重置方法：`reset(self)`

这个方法的作用是**把智能体重置到初始位置，开始新一局任务**,把`x`和`y`恢复到初始值（左下角起点），然后返回初始位置的一维状态值。。



```python
class Sarsa:
    """ Sarsa算法 """
    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格
        self.n_action = n_action  # 动作个数
        self.alpha = alpha  # 学习率
        self.gamma = gamma  # 折扣因子
        self.epsilon = epsilon  # epsilon-贪婪策略中的参数
    def take_action(self, state):  # 选取下一步的操作,具体实现为epsilon-贪婪
    if np.random.random() < self.epsilon:
        action = np.random.randint(self.n_action)
    else:
        action = np.argmax(self.Q_table[state])
    return action

def best_action(self, state):  # 用于打印策略
    Q_max = np.max(self.Q_table[state])
    a = [0 for _ in range(self.n_action)]
    for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来
        if self.Q_table[state, i] == Q_max:
            a[i] = 1
    return a

def update(self, s0, a0, r, s1, a1):
    td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
    self.Q_table[s0, a0] += self.alpha * td_error
```

核心变量：`self.Q_table`（Q 值表）

- 这是一个`numpy`二维数组，形状为 `[状态数, 动作数]`，其中「状态数 = nrow * ncol」（对应环境的二维坐标转一维状态）。

* 举例：4 行 12 列的悬崖行走环境，Q 值表形状为 `[48, 4]`（48 个状态，每个状态 4 个动作），`Q_table[0, 3]` 就表示「初始状态（状态 0）下，选择 “右” 动作（动作 3）的价值」。



```python
def update(self, s0, a0, r, s1, a1):    td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]    self.Q_table[s0, a0] += self.alpha * td_error
```

入参说明（对应 Sarsa 的 “一步试错” 数据）：

- `s0`：当前状态（执行动作前的状态）。
- `a0`：当前动作（在`s0`下执行的动作）。
- `r`：即时奖励（执行`a0`后得到的奖励）。
- `s1`：新状态（执行`a0`后进入的状态）。
- `a1`：新动作（在`s1`下用 ε- 贪心选好的下一个动作）。

逻辑拆解（对应 Sarsa 的核心更新公式）：

1. 计算TD 误差：

   ```python
   td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
   ```

   - 对应公式：误差
   - 含义：“当前动作的实际价值（当前奖励 + 下一个状态 - 动作的折扣价值）” 与 “之前估计的价值” 之间的差距，差距越大，Q 值更新幅度越大。

注意：Sarsa 是**在线算法**，这里用到了下一个状态`s1`对应的实际动作`a1`，这是它和 Q-learning 的核心区别。

更新 Q 值：`self.Q_table[s0, a0] += self.alpha * td_error`

- 对应公式：![image-20260202115855238](RL基本原理-img/image-20260202115855238.png)误差
- 含义：让`Q(s0,a0)`向 “实际价值” 靠近，`alpha`（学习率）控制靠近的幅度，避免更新过快导致震荡。





#### 多步 Sarsa 算法

蒙特卡洛方法利用**当前状态之后每一步的奖励**而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是**无偏**（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。那有没有什么方法可以结合二者的优势呢？答案是**多步时序差分**！多步时序差分的意思是使用步的奖励，然后使用之后状态的价值估计。用公式表示，将

![image-20260202151900107](RL基本原理-img/image-20260202151900107.png)

替换成

![image-20260202151916712](RL基本原理-img/image-20260202151916712.png)

于是，相应存在一种多步 Sarsa 算法，它把 Sarsa 算法中的动作价值函数的更新公式（参见 5.3 节）

![image-20260202151944550](RL基本原理-img/image-20260202151944550.png)

替换成

![image-20260202151958774](RL基本原理-img/image-20260202151958774.png)

我们接下来用代码实现多步（n步）Sarsa 算法。在 Sarsa 代码的基础上进行修改，引入多步时序差分计算。

```python
# 步骤2：若缓存列表长度达到n，进行n步更新（核心逻辑）
if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新
    G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})
    for i in reversed(range(self.n)):
        G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报
```

#### Q-learning 算法

除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为

![image-20260202161935549](RL基本原理-img/image-20260202161935549.png)

Q-learning 算法的具体流程如下：

![image-20260202162145211](RL基本原理-img/image-20260202162145211.png)

**Sarsa 是 “在线（on-policy）” 算法，更新时用的是真实选的下一个动作；而 Q-learning 是 “离线（off-policy）” 算法，更新时用的是 “下一个状态里价值最高的动作”，不管实际会不会选它**。

![image-20260202162527824](RL基本原理-img/image-20260202162527824.png)

而 Sarsa 估计当前**ε-贪婪**策略的动作价值函数。

需要强调的是，Q-learning 的更新并非必须使用当前贪心策略![image-20260202162927642](RL基本原理-img/image-20260202162927642.png)采样得到的数据，因为给定任意![image-20260202162946686](RL基本原理-img/image-20260202162946686.png)都可以直接根据更新公式来更新，为了探索，我们通常使用一个-贪婪策略来与环境交互。



#### 在线策略算法与离线策略算法

我们称采样数据的策略为**行为策略**（behavior policy），称用这些数据来更新的策略为**目标策略**（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。

Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，如图 5-1 所示。具体而言：

- 对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组，因此它是在线策略学习方法；

- 对于 Q-learning，它的更新公式使用的是四元组来更新当前状态动作对的价值，数据中的和是给定的条件，和皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。

![image-20260202163220148](RL基本原理-img/image-20260202163220148.png)

2. Sarsa（在线算法）的过马路例子

假设你正在学习过马路，Sarsa 的方式是：

1. **真实走一步**：你在马路边（状态`s`），用 ε- 贪心选了 “小步往前走”（动作`a`）。

2. **得到反馈**：走了一步后，没被车撞（奖励`r=+1`），到了马路中间（新状态`s'`）。

3. **再真实选下一步**：在马路中间，你又用 ε- 贪心选了 “继续小步走”（新动作`a'`）。

4. 更新经验

   ：你会用这一整套真实经历（

   ```
   s,a,r,s',a'
   ```

   五元组）来更新 “马路边 - 小步走” 的价值。

   - 你的经验是：“我在马路边选了小步走，到了中间，然后又选了继续走，所以这个动作的价值是`r + γ*Q(s',a')`”。
   - 你只相信自己真实走过的每一步，不参考任何 “想象中的最优走法”。

   

所以 Sarsa 是**在线算法**：它的更新依赖 “自己真实走出来的五元组”，每一步都是你实际执行的动作，非常保守、安全。

------

3. Q-learning（离线算法）的过马路例子

同样是学习过马路，Q-learning 的方式是：

1. **真实走一步**：你在马路边（状态`s`），用 ε- 贪心选了 “小步往前走”（动作`a`）。

2. **得到反馈**：走了一步后，没被车撞（奖励`r=+1`），到了马路中间（新状态`s'`）。

3. **想象最优走法**：你不关心下一步自己真实会选什么动作，而是直接看 “马路中间” 这个状态下，所有动作里价值最高的那个（比如 “快速跑过马路”，动作`a'`）。

4. 更新经验

   ：你会用 “真实走的一步 + 想象的最优下一步”（

   ```
   s,a,r,s'
   ```

   四元组）来更新 “马路边 - 小步走” 的价值。

   - 你的经验是：“我在马路边选了小步走，到了中间，而中间最好的动作是快速跑，所以这个动作的价值是`r + γ*maxQ(s',a')`”。
   - 你参考了 “理想中的最优走法”，哪怕你自己真实下一步可能不会这么选。

   

所以 Q-learning 是**离线算法**：它的更新只需要 “真实走的四元组”，下一步的动作是 “想象中的最优动作”，不需要是你真实选的，更激进、追求理论最优。



接下来仍然在悬崖漫步环境下来实现 Q-learning 算法。

```py
class QLearning:
    """ Q-learning算法 """
    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格
        self.n_action = n_action  # 动作个数
        self.alpha = alpha  # 学习率
        self.gamma = gamma  # 折扣因子
        self.epsilon = epsilon  # epsilon-贪婪策略中的参数
        def take_action(self, state):  #选取下一步的操作
    if np.random.random() < self.epsilon:
        action = np.random.randint(self.n_action)
    else:
        action = np.argmax(self.Q_table[state])
    return action

def best_action(self, state):  # 用于打印策略
    Q_max = np.max(self.Q_table[state])
    a = [0 for _ in range(self.n_action)]
    for i in range(self.n_action):
        if self.Q_table[state, i] == Q_max:
            a[i] = 1
    return a

def update(self, s0, a0, r, s1):
    td_error = r + self.gamma * self.Q_table[s1].max(
    ) - self.Q_table[s0, a0]
    self.Q_table[s0, a0] += self.alpha * td_error
```

逐句拆解更新逻辑

1. 计算 TD 误差：`td_error = r + self.gamma * self.Q_table[s1].max() - self.Q_table[s0, a0]`

   

   - 先拆各个部分的含义：
     - `self.Q_table[s1].max()`：获取新状态`s1`下，所有动作的 Q 值中的**最大值**（对应公式里的`max_{a'} Q(s',a')`），这是 Q-learning 的核心 —— 不关心真实选的动作，只取 “理想中的最优动作” 的 Q 值。
     - `self.gamma * self.Q_table[s1].max()`：对未来最优 Q 值打折扣（`gamma`是折扣因子，控制未来奖励的权重）。
     - `r + self.gamma * self.Q_table[s1].max()`：这是 Q-learning 的 “目标 Q 值”—— 当前奖励 + 未来最优状态的折扣 Q 值，代表 “这个动作的理想最优价值”。
     - `self.Q_table[s0, a0]`：这是当前「状态 s0 - 动作 a0」的 “当前 Q 值”—— 代表 “我们之前对这个动作的价值估计”。
     - `TD误差`：目标 Q 值 - 当前 Q 值，代表 “我们之前的估计和理想最优价值的差距”，差距越大，Q 值更新幅度越大。

比如你在起点（s0）选了 “右”（a0），得到奖励 - 1（r），到了新状态 s1，Q-learning 不管你下一步真实选什么，只看 s1 状态下最好的动作值多少钱，然后用这个 “理想价值” 来修正 “起点 - 右” 的价值。



需要注意的是，打印出来的回报是行为策略在环境中交互得到的，而不是 Q-learning 算法在学习的目标策略的真实回报。

我们把目标策略的行为打印出来后，发现其更偏向于走在悬崖边上，这与 Sarsa 算法得到的比较保守的策略相比是更优的。 但是仔细观察 Sarsa 和 Q-learning 在训练过程中的回报曲线图，我们可以发现，在一个序列中 Sarsa 获得的期望回报是高于 Q-learning 的。这是因为在训练过程中智能体采取基于当前函数的-贪婪策略来平衡探索与利用，Q-learning 算法由于沿着悬崖边走，会以一定概率探索“掉入悬崖”这一动作，而 Sarsa 相对保守的路线使智能体几乎不可能掉入悬崖。

#### 小结

本章介绍了无模型的强化学习中的一种非常重要的算法——时序差分算法。时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。

值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。

如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为**离线强化学习**（offline reinforcement learning），第 18 章将会介绍离线强化学习的相关知识。





### 6.Dyna-Q

在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：**基于模型的强化学习**（model-based reinforcement learning）和**无模型的强化学习**（model-free reinforcement learning）。

无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，第 5 章讨论的两种时序差分算法，即 Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法，本书在后续章节中将要介绍的方法也大多是无模型的强化学习算法。

在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。

第 4 章讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。











