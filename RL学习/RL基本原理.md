## RL基本原理

强化学习的两类思路：

一个是基于值，另一个是基于策略：

![image-20251125173841701](RL基本原理-img/image-20251125173841701.png)

### 时序差分(Temporal Difference)TD算法

我们的目的就是去找到一种方法能够`估计`在一个状态下做出动作，他的一个价值。

![image-20251125174327972](RL基本原理-img/image-20251125174327972.png)



![image-20251125185222517](RL基本原理-img/image-20251125185222517.png)

蒙特卡洛：是一长串多步估计，没有去考虑。

我玩完这一局游戏，我这里面不是有很多随机性吗，但我玩完它这个就确定下来，你没有去考虑它那么多期望、随机性，你就是相当于做了一个很多采样，我们知道很多变量它本身就有方差，如果把很多变量的方差叠加起来，他这个方差肯定是会很大的，但是因为你采取了这么一整条轨迹，我可以对它的return做一个很精确的一个计算，所以偏差可能会小。

![image-20251125190646846](RL基本原理-img/image-20251125190646846.png)

时序差分：相比于蒙特卡洛，他的方差会比较小，但是他的偏差可能就会比较大，因为它就是对一步做了一个估计。



TD算法有两种应用：一种叫SARSA，一种叫Q-learning





